# üçÑ Taller de Aprendizaje Autom√°tico No Supervisado con el Mushroom Dataset

## üåü Descripci√≥n y Objetivos

Este proyecto explora la estructura intr√≠nseca del **Mushroom Dataset** mediante t√©cnicas de Aprendizaje Autom√°tico No Supervisado.

Los objetivos clave son:

* Limpiar y preprocesar datos categ√≥ricos usando **One-Hot Encoding**.
* Reducir la dimensionalidad con **PCA** (An√°lisis de Componentes Principales).
* Aplicar **K-Means Clustering** para descubrir agrupaciones naturales ($K=2$).
* Comparar el rendimiento no supervisado (**ARI**) con una l√≠nea base supervisada (**Random Forest**).

---

## üõ†Ô∏è Tecnolog√≠as y Librer√≠as

El proyecto se desarroll√≥ en un entorno de **Jupyter/Google Colab** y requiere las siguientes dependencias:

### Requisitos de Python

| Categor√≠a | Librer√≠a | Prop√≥sito Principal |
| :--- | :--- | :--- |
| **N√∫cleo de Datos** | `pandas` | Limpieza, OHE y manipulaci√≥n de DataFrames. |
| **C√°lculo Num√©rico** | `numpy` | Manejo de arrays y valores faltantes (`np.nan`). |
| **Modelado ML** | `scikit-learn` | PCA, K-Means, RandomForest y M√©tricas de Evaluaci√≥n. |
| **Visualizaci√≥n** | `matplotlib`, `seaborn` | Generaci√≥n de gr√°ficos (PCA 2D, M√©todo del Codo). |

### Instalaci√≥n

Se recomienda instalar las dependencias en un entorno virtual:

```bash
pip install pandas numpy scikit-learn matplotlib seaborn

üöÄ Gu√≠a de Ejecuci√≥n
1. Carga y Preprocesamiento de Datos

El dataset se obtiene directamente desde el repositorio de la UCI:

# Carga directa del dataset (asumiendo column_names definido)
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data"
df = pd.read_csv(url, header=None, names=column_names)

# Comandos de Preprocesamiento Clave:
# 1. Eliminaci√≥n de columna 'veil-type' (constante) y filas con valores '?' en 'stalk-root'.
# 2. Aplicaci√≥n de One-Hot Encoding (OHE) a 21 columnas categ√≥ricas, resultando en 96 features.
X_encoded = pd.get_dummies(X, drop_first=False) 

# 3. Escalado de Datos (Fundamental para PCA y K-Means):
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_encoded)

```

¬°Aqu√≠ tienes esa secci√≥n del README en formato Markdown, con las tablas bien estructuradas!

Markdown

### 2. Modelado y Evaluaci√≥n

| Modelo / T√©cnica | Comando Principal | M√©trica de Evaluaci√≥n |
| :--- | :--- | :--- |
| **PCA** | `pca = PCA(n_components=2).fit_transform(X_scaled)` | Varianza Explicada. |
| **Random Forest (Baseline)** | `rf = RandomForestClassifier(...).fit(X_train, y_train)` | **Accuracy** (Precisi√≥n en test). |
| **K-Means Clustering** | `kmeans = KMeans(n_clusters=2, ...).fit_predict(X_scaled)` | **Adjusted Rand Index (ARI)**. |

---

## üìä Resultados y Conclusi√≥n Final

### Hallazgos de Rendimiento

| Tarea | Modelo | Resultado Clave | Implicaci√≥n |
| :--- | :--- | :--- | :--- |
| **Supervisado** | Random Forest | **Accuracy: $\approx 1.0000$** | El problema es trivialmente clasificable con etiquetas. |
| **No Supervisado** | K-Means | **ARI: $\approx 0.99 - 1.00$** | Coincidencia casi perfecta con las clases reales. |
| **Reducci√≥n** | PCA (2D) | Separa las clases de forma visible. | La informaci√≥n crucial se concentra eficientemente. |

### Conclusi√≥n

El an√°lisis demuestra que la estructura inherente del **Mushroom Dataset** es fuertemente binaria. La alta puntuaci√≥n del **Adjusted Rand Index (ARI)** valida que el **K-Means Clustering** fue capaz de **descubrir la clasificaci√≥n** (comestible vs. venenoso) sin utilizar las etiquetas de entrenamiento. Esto subraya la potencia de la combinaci√≥n de One-Hot Encoding, PCA y K-Means para el an√°lisis y segmentaci√≥n de datos categ√≥ricos en el √°mbito no supervisado.

<p align="center">Desarrollado con ‚ù§Ô∏è usando Python. **SparkCode** ‚ú®</p>